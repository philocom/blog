---
layout: post
title: (번역) Neural networks and Deep learning - 1장 (2)
category: neural networks and deep learning
tags: [번역, 1장]
---
-**원저자: [Michael Neilson](http://michaelnielsen.org/)**<br>
-**원문주소: [http://neuralnetworksanddeeplearning.com/chap1.html](http://neuralnetworksanddeeplearning.com/chap1.html)**<br>
-**역자: [galji(지중현)](joonghyunji@gmail.com)**<br>
***본 번역의 무단 전재 및 재배포를 금지합니다.***
<br>
<br>
$\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$

##6. 경사하강법을 통학 학습##

우리가 설계한 뉴럴네트워크를 가지고 어떻게 숫자인식을 학습시킬 수 있을까? 여기서 맨 처음 훈련데이터집합 (training dataset)이라고 불리는 데이터집합이 필요해진다. 우리는 [MNIST 데이터집합](http://yann.lecun.com/exdb/mnist/)을 사용한다. 이것은 수만개의 스캔된 손글씨 숫자 이미지들을 포함하고 있으며 각 이미지에 대하여 올바른 숫자가 분류되어 있다. 참고로, MNIST 데이터집합은 [NIST(미국국립표준기술연구소)](https://www.google.co.kr/search?q=nist&ie=utf-8&oe=utf-8&gws_rd=cr&ei=DYCkVrmTNObgmAX5jq6oDQ)에서 만든 것이다.

![handwritten digits](/blog/assets/images/digits_separate.png){: .center-image}

이 숫자들은 사실 이번 장의 처음에서 보았던 숫자들과 같다는 걸 알아차렸을 것이다. 물론, 우리가 만든 네트워크를 테스트할때 훈련집합에 있는 이미지들을 사용하지는 않을 것이다.

MNIST 데이터는 두 개의 파트로 나뉜다. 첫째 파트는 훈련데이터로서 쓰이는 60,000개의 이미지를 포함한다. 이 이미지들은 250명의 사람들의 손글씨를 일일이 스캔하여 얻은 샘플들이다. 이 과정에서 미통계청(US Census Bureau) 직원들과 고등학교 학생들의 손글씨가 각각 절반씩 담겨있다.  스캔된 모든 이미지는 회색조로서 28x28픽셀의 크기이다. 둘째 파트는 10,000개의 이미지를 시험 데이터(test data)로서 10,000장의 이미지를 포함한다. 마찬가지로, 여기 포함된 이미지 모두 회색조에 28x28픽셀의 크기이다.  우리의 뉴럴네트워크가 얼마나 숫자인식을 잘 학습하는지 알아보기 위해 시험 데이터를 사용할 것이다. 좋은 성능 테스트가 되기 위해서, 시험 데이터는 원본 훈련데이터를 분류하지 않은 다른 250의 사람들로부터 뽑았다. 이것은 우리의 시스템이 훈련단계에서 보지못한 (다른 사람이 쓴) 숫자를 인식할 때 신뢰성 측면에서 도움이 된다.

여기서, 훈련입력(training input)을 $x$로 표기하겠다. 훈련입력 $x$는 $28 \times 28 = 784$-차원의 벡터로 간주하는 것이 편리하다. 벡터의 각 원소는 이미지 픽셀 한개의 회색조 밝기값을 나타낸다. 우리가 여기서 얻고싶은 출력 $y$는 10차원 벡터로서 $y=y(x)$로 표현할 것이다. 예를 들어 만약 특정 훈련(입력) 이미지 $x$가 숫자 6을 나타내는 것이라면, $y(x)=(0,0,0,0,0,0,1,0,0,0)^T$가 네트워크에서 요구되는 출력이다. 여기서, $T$는 전치연산으로서 행벡터를 열벡터로 바꾸어준다.

우리가 원하는 것은 모든 훈련인풋 $x$들에 대하여 네트워크 출력이 $y(x)$와 비슷해지도록 가중치들과 편향치를 찾아내는 알고리즘이다. 이 목표를 얼마나 잘 달성하는지 수치화할 수 있도록 '비용함수'를 정의해보자:

$$
\begin{eqnarray}
\label{eq:6}
  C(w,b) \equiv
  \frac{1}{2n} \sum_x \| y(x) - a\|^2.
\tag{6}\end{eqnarray}
$$

여기에서, $w$과 $b$는 각각 네트워크에 있는 모든 가중치와 편향치들을 대표하는 기호이다. $a$는 네트워크에서 $x$가 입력일때 훈련출력(training output) 벡터를 가리킨다. 물론, 출력 $a$는 $x,w$ 그리고 $b$ 모두에 대해 변하는 값이지만, 간편한 표기를 위해 생략했다. $\|v\|$ 표기는 벡터 $v$의 길이를 나타낸다. 우리는 위의 $C$함수를 '이차(quadratic) 함수'라고 부를 것이다; 이것은 평균제곱오차를 나타내는 MSE(mean squared error)라고도 부른다. (이것을 이용하여 만든 수식 $\eqref{eq:6}$는 각각의 입력 $x$에 대한 오차를 모두 누적하여 더한 후 제곱하여 정규화한 값이라고 보면 된다.) 이차 비용함수의 형태를 조사해보면, 각 입력에 대한 오차들을 합산할때 개별적인 오차들도 음수가 아니므\로 $C(w,b)$도 음수가 아님을 알수 있다. 또한, $y$가 모든 훈련입력들 $x$에 대하여 출력 $a$와 거의 비슷해질 때, 비용 $C(w,b)$는 $0$과 비슷해질 정도로 작아진다. 그래서, 우리의 훈련 알고리즘(training algorithm)은 $C(w,b)\approx 0$를 만족하는 가중치들과 편향치들을 찾을수만 있다면 잘 작동하는 것이다. 반대로,  $C(w,b)$값이 커진다면 $y(x)$가 출력 $a$와 가깝지 않음을 의미하므로 알고리즘이 작동하지 않는다고 볼 수 있다. 우리의 훈련 알고리즘은 목표는 가중치와 편향치에 관하여 비용함수 $C$의 값을 최소화하는 것이다. 다시 말하자면, 이 비용을 가능한 작게 만들 수 있는 가중치들과 편향치들의 집합을 찾는 것이다. 우리는 '경사하강법(gradient descent)'이라고 알려진 알고리즘을 사용하여 이 목표를 달성할 것이다.    

왜 이차비용을 도입해야 할까? 우리는 무엇보다 정확하게 분류된 이미지의 숫자를 늘리는데 주된 관심이 있지 않은가? 그냥 그 숫자를 최대화시키는 것은 안되는 걸까? 이차비용같은 대용물(proxy)의 크기를 최소화하는 방법을 쓰지 않고 말이다. 여기에는 문제가 있다. 잘 분류된 이미지의 갯수는 가중치들과 편향치들의 매끄러운 함수가 아니기 때문이다. 대부분의 경우, 가중치들와 편향치들을 살짝 바꾸는 것이 정확하게 분류된 훈련이미지의 갯수에 별 영향을 미치지 않는다. 이 사실은 성능 향상을 위해 어떻게 가중치들과 편향치들을 바꿀지 알아내는 것을 어렵게 한다. 그 대신에 이차비용같은 매끄러운 비용함수를 사용한다면, 비용을 절감하기 위해서 가중치들과 편향치을 어떻게 변화시킬지 알아내기 쉽다. 따라서 우리는 이차비용을 최소화하는 데 처음 촛점을 맞출 것이고, 분류의 정확도 문제는 다음에 고찰할 것이다.


목적을 위해 매끄러운 비용함수를 써야한다는 것을 알게 되었다. 하지만 아직도 수식 $\eqref{eq:6}$같은 비용함수를 왜 써야하는지 의아해 할지 모른다. 이것은 임시방편(*ad hoc*)적인 선택이지 않을까? 만약 다른 비용함수를 고른다면, 어쩌면 최소비용을 주는 가중치들과 편향치들이 완전히 다르지 않을까?  이것은 근거있는 걱정이다. 그래서 우리는 비용함수를 살짝 변경해보는 방법으로 나중에 이 문제를 다시 살펴볼 것이다. 그렇지만, 수식  $\eqref{eq:6}$같은 이차비용 함수는 뉴럴네트워크 학습의 기초를 이해하기 위해서 완벽하게 잘 작동한다. 따라서 당분간은 이것을 계속 사용할 것이다.

다시 요약하자면, 뉴럴네트워크 훈련에서 우리의 목표는 이차비용 함수 $C(w,b)$를 최소화하는 가중치들과 편향치들을 찾는 것이다. 이것은 적절하게 정립된(well-posed) 문제이지만, 현재 이 문제를 정립할 때 집중을 방해하는 구조들이 많이 있다. 이것들은 가중치들과 편향치로서 $w$와 $b$에 대한 해석, 배경에서 숨어있는 $\sigma$ 함수, 사용할 네트워크 구조에 대한 선택 그리고 MNIST등을 포함한다. 앞서 말한 구조들 대부분을 무시하고 최소화 관점에서만 집중하므로서 이해의 폭을 넓힐 수 있다. 그래서 지금부터는 비용함수의 특정 형태에 관한 모든 것은 잊어버려라. 대신에 많은 변수들로 이루어진 하나의 함수를 제시하고 그 함수를 최소화하길 원한다고 가정하자. 우리는 이러한 최소화 문제들을 푸는데 사용할 수 있는 하나의 기법인 '경사하강법'을 전개할 것이다.  그 다음에 비로소 뉴럴네트워크에 있는 특정 비용함수를 최소화는 문제로 돌아가기로 하자.

좋다. 이제 어떤 함수 $C(v)$를 최소화하고 싶다고 가정해 보자. 이 함수는 여러개의 변수들로 이루어진 실수의 함수로서 $v=v_1,v_2,\ldots$로 표기한다. 어떤 함수라도 될 수 있으므로 $w$와 $b$ 대신에 $v$으로 표기를 대체하였음을 주의하라 - 특정한 뉴럴네트워크 맥락에서만 염두하는 것이 아니기 때문이다. $C(v)$를 최소화하기 위해 $C$가 두 변수 $v_1$과 $v_2$의 함수로 가정하자:

![valley.png](/blog/assets/images/valley.png){: .center-image}

우리가 찾으려고 하는 것은 $C$가 전역적 최소값(global minimum)을 갖는 위치이다. 물론, 위에서 플롯된 함수에 대하여 그래프를 응시하면 최소값을 찾을 수 있다. 그런 점에서, 어쩌면 지금 보고 있는 것이 너무 간단한 함수일지도 모른다. 일반적인 함수 $C$는 많은 변수들로 이루어진 복잡한 변수일 수 있으므로 그래프를 응시해서 최소값을 눈대중으로 찾아내는 것은 보통 가능하지 않기 때문이다.  

이 문제를 해결하기 위한 한 방법은 미적분을 이용해서 해석적 최소값(analytic minumum)을 찾는 것이다. $C$가 극소 혹은 극대가 되는 위치를 찾기 위해 도함수들을 계산하여 사용하는 것이다. $C$ 함수가 단지 한개 혹은 몇개의 변수들로 이루어져 있다면, 운이 좋은 경우 답을 잘 찾아내기도 한다. 그러나 많은 변수들을 포함할 경우 이것은 악몽이다. 예를 들어, 뉴럴네트워크는 훨씬 더 많은 변수들을 사용하는 함수를 가지고 있다. 어떤 뉴럴네트워크는 수십억개의 가중치들과 편향치들에 의존하는 비용함수를 가지고 극도로 복잡한 방식으로 작동한다. 미적분을 이용해서 그런 함수의 최소값을 찾는 것은 불가능하다!

($C$가 단지 두개의 변수로 이루어진 함수라고 상상함으로서 약간의 통찰력을 얻게되리라 단언한 후에, 나는 앞서 두 단락의 내용을 두번이나 되돌아 보고 말했다. "이봐, 하지만 만약 두 변수가 아닌 훨씬 더 많은 변수의 함수라면 어떤가?" 정말 미안하다. 하지만, 나는 두 변수로 이루어진 $C$ 함수로 가정하는 것이 정말 도움이 될 거라고 믿어주기를 바란다. 때로는 그런 가정이 실패하므로, 앞선 두 단락이 그런 실패에 대하여 다루고 있다. 수학에 관해 올바르게 사고하는 것이라 함은 때때로 여러개의 직관적인 가정들을 요리조리 재보고, 각 가정을 사용하기 적절한 경우와 그렇지 않은 경우를 배우는 것이다.)

알았다. 그렇다면 미적분으로는 해결할 수 없는것이다. 다행이 꽤 잘 작동하는 알고리즘을 만들 수 있는 하나의 아름다운 비유가 있다. 아까 그 함수를 골짜기 같은 종류로 비유하는 것이다. 위의 그래프를 약간 자세히 관찰하면 그건 그리 어렵지 않다. 이 골짜기의 경사면에서 굴러내려오는 공을 하나 생각해보자. 우리의 경험에 의하면 이 공은 결국 골짜기의 맨 아래로 굴러갈 것이다. 어쩌면 이 아이디어를 이용해 함수의 최소값을 찾을 수 있지 않을까? 먼저 그 상상속의 공의 처음 위치를 무작위로 정한다. 그리고 나서 공이 골자기의 맨 아래로 내려갈때 그 공의 움직임을 시뮬레이션 하는 것이다. 우리는 $C$의 일차 도함수를 계산하여 (그리고 추가적으로 이차 미분값을 계산하여) 쉽게 이런 시뮬레이션을 할 수 있다. 이러한 도함수들은 골짜기의 국부적 '모양'에 대해 우리가 알아야 할 모든 것을 알려준다. 따라서, 공이 어떻게 굴러가는지 알 수 있다.

내가 방금 말한 것에 근거하여, 우리는 바닥면의 마찰과 중력의 효과를 고려한 공에 대한 뉴턴의 운동방정식을 적어볼 수 있을 것이다. 실제로, 우리는 공굴리기의 비유를 그렇게 심각하게 취하진 않을 것이다. 그저 $C$값을 최소화할 수 있는 알고리즘을 고안하는 것이지, 물리법칙을 정교하게 시뮬레이션하는 알고리즘을 고안하는 것이 아니다. 공의 시각에서 보는 것은 우리의 상상력을 국한시키지 않니라 오히려 자극시킨다. 따라서 물리학의 잡다한 모든 세부사항으로 들어가기 보다는 우리자신에게 이렇게 간단히 물어보자: 만약 우리가 하루동안 신이된다면, 그리고 공이 어떻게 굴러가야하는지를 구술하는 운동법칙을 만들 수 있다고 생각해 보자. 그렇다면 공을 언제나 골짜기 맨 아래로 굴러가게 하기위해 우리는 어떤 법칙이나 운동법칙을 골라야 할까?

이 질문을 좀 더 정교하게 만들기 위해서 우리가 공을 $v_1$방향으로 $\Delta v_1$만큼 그리고 $v_2$방향으로 $\Delta v_2$만큼 소량으로 움직인다면 무슨 일이 일어날지 생각해 보자. 미적분은 $C$가 다음과 같이 변한다고 말해준다:

$$
\begin{eqnarray}
\label{eq:7}
  \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +
  \frac{\partial C}{\partial v_2} \Delta v_2.
\tag{7}\end{eqnarray}
$$

$\norm{aaa}$, $\norm{aaa}$.

$\Delta C$를 음수로 만들기 위해 $\Delta v_1$과 $\Delta v_2$를 선택하는 방법을 찾으려고 한다. 다시 말해, 공이 골짜기 아래로 굴러가도록 하도록 그 값들을 정할 것이다. 그런 선택을 어떻게 만들지 알아내기 위해서, 우선 $\Delta v$라는 벡터를 정의하는 데 이것은 $v$에서의 변화를 나타내며 $\Delta v \equiv (\delta v_1, \delta v_2)^T$와 같다. 또한, $C$의 경사도(gradient)는 편미분 도함수인 $(\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2})^T$로 정의할 수 있다. 이 경사도 벡터는 다음과 같이 $\nabla C$으로 표기한다:

$$
\begin{eqnarray}
\label{eq:8}
  \Delta C \approx \nabla C \cdot \Delta v.
\tag{9}\end{eqnarray}
$$

곧 변화를 나타내는 $\Delta C$ $\Delta v$와 $\nabla C$로 다시 쓸 수 있을 것이다. 그렇게 하기전에 경사도의 개념에 대해 무척 어려워하는 사람들을 위해 몇가지 사항을 명백하게 알려주고 싶다. $\nabla C$ 표기를 처음 접할 때, 사람들은 도대체 $\nabla$ 기호가 무엇을 뜻하는지 궁금해한다. $\nabla$가 정확히 무엇을 뜻하는가? 사실, $\nabla$를 앞서 두 개의 기호를 이용하여 정의한 수식처럼 그저 하나의 수학적 객체로서 간주하는 것은 완벽하게 옳다. 이러한 관점에서는 $\nabla$는 단지 표기상의 약속만 나타낼 뿐으로 이렇게 말하고 있다. "이봐, $\nabla$는 경사도 벡터야."  $\nabla$를 독립적인 수학적 개체로서 미분 연산자처럼 간주하는 고급의 관점도 있지만, 여기서는 그런 관점까지는 필요없을 것이다.

이러한 정의들을 가지고, $\Delta C$에 대한 수식 $\eqref{eq:7}$을 아래와 같이 고쳐보자:

$$
\begin{eqnarray}
\label{eq:9}
  \Delta C \approx \nabla C \cdot \Delta v.
\tag{9}\end{eqnarray}
$$

위의 수식은 왜 $\nabla C$가 경도사 벡터로 불리는지를 설명하기 편리하게 해준다: $\nabla  C$는 $v$의 변화를 $C$의 변화와 연결시켜 준다. 이것이 우리가 경사도라는 용어로부터 기대한 것이다. 하지만 위 수식에서 가장 흥미로운 부분은 이것이 $\Delta C$를 음수로 만들기 위해 $\Delta v$를 어떻게 선택할지 알게 해준다는 점이다. 특별히 아래와 같이 $\Delta v$를 선택한다고 하자.


$$
\begin{eqnarray}
\label{eq:10}
  \Delta v = -\eta \nabla C,
\tag{10}\end{eqnarray}
$$

이때, $\eta$는 작은 양수의 매개변수(parameter)이며 학습률(learning rate)이라고도 알려져 있다. 그렇다면 \eqref{eq:9}는 $\Delta C$ $\approx$ $-\eta \nabla C \cdot \nabla C$ $=$ $-\eta \l|\nabla C\|^2$임을 나타내는 것이다. $\| \nabla C\|^2 \geq 0$이므로 $\Delta C \leq 0$을 보장한다. 다시 말해, 수식 $\eqref{eq:10}$에 따라서 $v$를 변화시킬 때 \$C$는 언제나 감소할 뿐 증가하지 않는다는 것이다. (물론 이것은 수식 $\eqref{eq:9}$로 근사한 경계안에서만 그렇다는 이야기이다.) 그렇다면, 수식 $\eqref{eq:10}$을 가지고 우리의 경사하강법에서 공의 "운동법칙"을 기술해보자. 즉, 수식 $\eqref{eq:10}$이 $\Delta v$의 값을 계산하고, 그 다음에 공의 위치 $v$를 $\Delta v$만큼 움직이는 것이다.

$$
\begin{eqnarray}
\label{eq:11}
  v \rightarrow v' = v -\eta \nabla C.
\tag{11}\end{eqnarray}
$$

그리고 나서, 또다른 움직임을 만들기 위해 이 갱신 규칙을 다시 이용할 것이다. 이것을 계속 반복하게 되면 $C$의  값을 계속해서 줄일 수 있고, 마침내 전역적 최소값에 닿을 수 있을 것이다.

요약하자면, 경사하강 알고리즘이 작동하는 방식은 경사도 $\nabla C$를 계산한 후 골짜기의 비탈로 "떨어지는" $\nabla C$의 반대부호 방향으로 움직이을 반복적으로 수행하는 것이다. 이 알고리즘을 시각화하면 다음과 같다:

 ![valley_with_ball](/blog/assets/images/valley_with_ball.png){: .center-image}


이런 규칙으로 경사하강법이 진짜 물리적인 운동을 묘사해내는 것이 아님을 주의하라. 현실에서 공은 운동량을 가지고 있고, 그 운동량은 공이 비탈을 가로질러 가거나 심지어 오르막길까지 이르게 할지 모른다. 공이 골짜기로 굴러 내려가는 것이 보장할 수 있는 전부이고 그 마저 마찰력에 의해 계속 영향을 받는다. 그와 대조적으로, 우리가 $\Delta v$를 선택하기 위해 만든 규칙은 단지 "내려가, 당장" 이라고 말한다. 그건 최소값을 찾는데 여전히 꽤 괜찮은 규칙이다!

경사하강법이 정확하게 작동하게 하고 싶다면, 학습률 $\eta$는 수식 \eqref{eq:9}가 잘 근사할 조건으로서 충분히 작은 숫자가 되어야 한다. 그렇지 않다면, $\Delta C > 0$이 되는데 이건 당연히 좋지 않다! 또한 $\eta$가 너무 작으면 안된다. $\eta$가 너무 작으면 $\Delta v$ 또한 매우 작게 만들기 때무에 경사하강 알고리즘이 굉장히 느리게 작동하게 되기 때문이다. 실제 구현에서는 수식 $\eqref{eq:9}$를 좋은 근사식으로 써먹을 수 있도록 그리고 알고리즘이 너무 느려지지 않도록 하기 위해 $\eta$를 자주 변경하게 된다. 이것이 어떻게 작동하는지 나중에 살펴보도록 하자.   

나는 $C$가 두 변수의 함수일때 경사하강법을 살펴보았다. 사실, $C$가 훨씬 더 많은 변수들의 함수일때도 같은 방식으로 작동한다. $C$가 $m$개의 변수들을 가진 함수라고 하자. 그러면 약간의 변화 $\Delta v = (\Delta v_1,
\ldots, \Delta v_m)^T$에 의하여 $C$의 변화인 $\Delta C$는 다음과 같다.

$$
\begin{eqnarray}
\label{eq:12}
  \Delta C \approx \nabla C \cdot \Delta v,
\tag{12}\end{eqnarray}
$$

여기서 경사도 $\nabla C$는 다음 벡터와 같다.

$$
\begin{eqnarray}
\label{eq:13}
  \nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots,
  \frac{\partial C}{\partial v_m}\right)^T.
\tag{13}\end{eqnarray}
$$

두 변수의 사례에서 본 것처럼, 우리는 $\Delta v$도 다음처럼 나타낼 수 있다.

$$
\begin{eqnarray}
\label{eq:14}
  \Delta v = -\eta \nabla C,
\tag{14}\end{eqnarray}
$$

또한 우리는 수식 $\eqref{eq:12}$에서 $\Delta C$가 음수가 됨을 보장할 수 있다. 이것이 최소값에 대한 경사도를 따라가는 방법이 된다. 심지어 이것은 $C$가 수많은 변수들의 함수라고 할 때도 갱신 규칙을 반복해서 적용하면 다음과 같이 최소값으로 가게 된다.

$$
\begin{eqnarray}
\label{eq:15}
 v \rightarrow v' = v-\eta \nabla C.
\tag{15}\end{eqnarray}
$$

이 갱신 규칙을 경사하강 알고리즘을 정의하는 것으로 봐도 무방하다. 이것은 위치 $v$를 반복적으로 변화시키면서 함수 $C$의 최소값을 찾는 방법이다. 이 규칙이 항상 작동하는 것은 아니다 - 몇몇 상황에서는 잘못된 경로로 가게되므로 경사하강법이 $C$의 전역적 최소값을 찾는 것을 방해할 수 있다. 이 점에 관해 따르는 장들에서 논의할 것이다. 하지만, 실제상황에서 경사하강법은 종종 굉장히 잘 작동하며, 그것이 뉴럴네트워크에서 비용 함수를 최소화하고 네트워크 학습을 도와주는데 강력한 방법임을 알게될 것이다.

경사하강법이 최소값을 찾는데 최적의 전략이라는 것은 정말로 일리가 있다. $C$를 가능한한 많이 감소시킬수 있는 방향으로 $\Delta v$를 움직이도록 한다고 가정하자. 이것은 $\Delta C \approx \nabla C \cdot \Delta v$를 만족하도록 최소화하는 것과 같다. 여기에서는 어떤 작은 양의 고정된 숫자 $\epsilon$에 대하여 $\|\Delta v \| = \epsilon$이 되도록 움직임의 크기를 제한할 것이다. 다시 말해서, 우리는 고정된 작은 간격크기 (step size)의 움직임을 원하며, $C$를 가능한한 많이 감소시킬 수 있는 운동방향을 찾으려 하는 것이다. 그것은 다음에 의해 증명가능하다. $\nabla C \cdot \Delta v$ 를 최소화하는 $\Delta v$의 선택은 간격크기 제한조건 $\|\Delta v\| = \epsilon$에 의해 결정되는 $\eta = \epsilon / \|\nabla C\|$을 만족하는 $\Delta v = - \eta \nabla C$과 같다라고 말이다. 따라서, 경사하강법은 $C$를 즉시 감소시키는데 가장 크게 기여하는 방향에서 작은 간격들을 밟아나가는 방법으로 볼 수도 있다.
<br><br>

###연습문제###

- 마지막 단락에서의 주장을 증명해라. 만약 여러분이 [코시슈바르츠 부등식(Cauchy-Schwarz inequality)](https://ko.wikipedia.org/wiki/%EC%BD%94%EC%8B%9C-%EC%8A%88%EB%B0%94%EB%A5%B4%EC%B8%A0_%EB%B6%80%EB%93%B1%EC%8B%9D)에 익숙치 않다면, 지금이 그것에 관해 익숙해질 기회이다.
<br>

- 나는 $C$가 두 변수 및 다변수의 함수가 될때 경사하강법을 설명했다. $C$가 만약 단일한 변수의 함수일 경우에는 어떤 일이 일어나는가? 1차원 경우에서는 경사하강법이 무엇을 하는지 기하학적인 해석을 내보아라.



















































































































ㅁㄴㅇㄻㅇㄴㄹ

ㅁㄴㅇㄹ
ㅁㄴㅇㄹ
ㅁㄴㅇㄹ
ㅁㄴㅇ
ㄻ
ㄴㅇㄹ
ㅁ

ㅁㄴㄹ
ㅁㄴ
ㅇㄻ
ㄴㅇㄹ
ㅁㄴ
ㅇㄹ
ㅁㄴ
ㄻㄴ
ㄹ
ㅁㄴㅇㄹ
ㄴㅁ
ㅇㄻ
ㄴㅇㄹ
ㅁㄴ
ㅇㄻㄴ
ㄹ
ㅁㅇㄴㄹ
ㅁㄴ
ㅇㄹ
ㅁㄴㄹ
ㅁㄴ
ㄻㄴ
ㅇㄻㅇ
ㄴㅇㄹ
ㅁㄴ
ㄻㄴ
ㅇㄻㄴ
ㅇㄹ
ㅁㄴㅇㄹ
ㅁㄴㅇㄹ
ㅁㄴ
ㅇㄻ
ㄴㄻ
ㄴㄻ
ㄴ



ㅁㄴㅇㄹ

ㄴㅁㅇㄹ



ㅁㄴㅇㄹ



ㄴㅁㅇㄹ





ㅁㄴㅇㄹ




ㄴㅁㅇㄹ
