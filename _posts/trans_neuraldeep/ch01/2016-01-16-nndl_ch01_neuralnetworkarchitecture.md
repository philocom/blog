---
layout: post
title: (번역) Neural networks and Deep learning - Ch1. 뉴럴네트워크로 손글씨 숫지를 인식하기 - 4부
tags: [neural network, deep learning, 번역, 1장]
---
-**원저자: [Michael Neilson](http://michaelnielsen.org/)**<br>
-**원문주소: [http://neuralnetworksanddeeplearning.com/chap1.html](http://neuralnetworksanddeeplearning.com/chap1.html)**<br>
-**역자: [galji(지중현)](joonghyunji@gmail.com)**<br>
***본 번역의 무단 전재 및 재배포를 금지합니다.***
<br>
<br>

###뉴럴네트워크 구조###

다음 절에서 나는 손글씨 숫자들을 꽤 잘 분류할 수 있는 뉴럴네트워크를 소개할 것이다. 따라서, 이 네트워크에서 각 부분들에 대한 이름을 붙이는 것이 용어들을 설명하는데 도움이 될 것 같다. 아래 그림과 같은 네트워크가 있다고 생각해 보자:

 ![handwritten digits](/blog/assets/images/tikz10.png){: .center-image}

이 네트워크에서 가장 왼쪽에 있는 층은 앞서 언급했듯이 입력층이라고 부르며, 입력층 안에 있는 뉴런들을 입력뉴런이라고 부른다. 가장 오른쪽 층은 출력층이라고 부르며 출력뉴런들을 포함한다 (여기서는 단 하나의 출력뉴런이 있다). 중간 층은 입력층이나 출력층도 아니므로 은닉층(hiden layer)라고 부른다. "은닉"이라는 용어가 조금 신비하게 들릴지 모른다. 이 용어를 처음 들었을때, 나는 이것에 철학적인 혹은 수학적인 심오함이 있을거라 생각했다). 그러나 은닉이라는 단어는 입력도 출력도 아닌 것을 의미할 뿐이다. 위에 예시된 네트워크는 한 개의 은닉층이 있지만 여러 개의 은닉층도 가능하다. 예를 들어, 아래의  4층짜리 네트워크에서 은닉층은 두 개이다.

 ![handwritten digits](/blog/assets/images/tikz11.png){: .center-image}

역사적인 이유로 위와 같은 시그모이드 뉴런 네트워크를 다층 퍼셉트론 (multilayer perceptron)이나 MLP라는 이름으로 부르기도 한다. 사실, 좀 혼동스러울 것이다. 이 책에서는 혼란을 피하기 위해 MLP라는 용어를 쓰지 않겠다. 그러나 이러한 용어가 쓰이고 있다는 사실은 알고 있어야 한다.

뉴럴네트워크의 입력층과 출력층을 설계(design)하기는 쉬워보인다. 예를들면, 어떤 손글씨 숫자를 "$9$"인지 판별하고 싶다고 가정해 보자. 가장 자연스러워 보이는 방법은, 이미지에서 픽셀 밝기를 입력뉴런으로 부호화(encoding)하여 네트워크를 만드는 것이다. 만약, 이미지가 $64 \times 64$ 크기의 회색조(greyscale) 이미지라면, $64 \times 64$개, 즉 $4096$개의 입력뉴런이 필요하다. 여기서의 밝기는 원본 밝기를 $0$에서 $1$ 사이로 정규화(normalization)한 것이다. 이 출력층에는 하나의 뉴런만 포함하고 있다. 만약, 출력값이 $0.5$보다 작으면 "입력 이미지는 $9$가 아니다"라고 가리킬 것이다. 그리고 출력값이 $0.5$보다 크면 "입력 이미지는 $9$다"라고 가리킬 것이다.

뉴럴네트워크에서 입력층이나 출력층을 설계하는 것은 꽤 쉬운 반면에, 은닉층을 설계하는 것은 예술이라고 말할 수 있을 정도로 어렵다. hidden layer를 디자인 하는것은 꽤나 어려워 보인다. 특히, 경험으로 얻은  몇개의 규칙만으로 은닉층을 전체적으로 설계하기는 불가능하다. 대신에, 뉴럴네트워크 연구자들은 네트워크를 원하는 방향으로 움직이게끔 도와주는 다양한 설계 휴리스틱(design heuristic)을 개발했다. 예를 들어, 이러한 휴리스틱은 두 개의 변수, 즉 네트워크가 학습하는데 소요되는 시간과 은닉층 갯수의 균형을 어떻게 맞출 지 결정한다. 그런 몇 가지 설계 휴리스틱은 나중에 살펴볼 것이다.  

우리는 지금까지 한 층의 출력이 이어지는 층의 입력이 되는 뉴럴네트워크에 대해 논의하고 있다. 이 뉴럴네트워크를 *전향(feedforward)* 네트워크라 부른다. 전향 네트워크는 루프(loop)가 존재하지 않으므로 정보의 반복적인 순환이 없으며, 정보를 앞쪽 방향으로만 전달하고 뒤쪽 방향으로는 전달하지 않는다.  그런 루프가 존재한다면, $\sigma$함수에 대한 입력이 $\sigma$함수에 대한 출력으로부터 다시 영향받는 상황도 그려볼 수 있다. 이런 상황은 타당하지 않으므로 루프는 허용되지 않는다.

그러나, 어떤 뉴럴네트워크 모델은 피드백 루프가 가능하기도 하다. 이런 모델은 재귀 (recurrent) 뉴럴네트워크라고 한다. 이 모델의 아이디어는 진행이 중단될때까지 제한된 시간동안 계속 발화하는 뉴런을 모방하는 것이다. 여기서, 뉴런이 발화하면 그 다음 뉴런들이 자극을 받아서 조금 늦게 그리고 마찬가지로 제한된 시간동안 계속 발화한다. 이것이 많은 뉴런들이 여전히 발화하는 원인이 되므로 시간이 흐르면서 뉴런의 발화는 폭포처럼 쏟아진다.  

재귀 뉴럴네트워크는 전향 뉴럴네트워크에 비해 덜 유명하다. 그 이유는 부분적으로 지금까지 재귀 네트워크에 대한 학습 알고리즘이 덜 강력했기 때문이다. 하지만, 재귀 네트워크는 아직도 굉장히 흥미롭다. 우리의 뇌가 작동하는 방식이 전형 네트워크보다 더 닮아있기 때문이다. 재귀 네트워크가 전형 네트워크가 풀기 대단히 어려운 중요한 문제를 쉽게 해결할 수도 있다. 그러나, 이 책에서는 일반적으로 많이 쓰이는 전형 네트워크로 범위를 한정하고 집중하기로 하자.   
