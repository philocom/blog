---
layout: post
title: (번역) Neural networks and Deep learning - 1장 (1)
category: neural networks and deep learning
tags: [번역, 1장]
---
-**원저자: [Michael Neilson](http://michaelnielsen.org/)**<br>
-**원문주소: [http://neuralnetworksanddeeplearning.com/chap1.html](http://neuralnetworksanddeeplearning.com/chap1.html)**<br>
-**역자: [galji(지중현)](joonghyunji@gmail.com)**<br>
***본 번역의 무단 전재 및 재배포를 금지합니다.***
<br>
<br>

##1. 뉴럴네트워크로 손글씨 숫자를 인식하기##

인간의 시각시스템은 이 세상에서 경이로운 것 중 하나이다. 아래의 필기체 숫자를 잠시 보자.

![handwritten digits](/blog/assets/images/digits.png){: .center-image}

사람들은 이 숫자를 별 노력 없이 504192라고 인식할 것이다. 그런데 그 원리는 사실 복잡하다. 인간의 좌뇌와 우뇌에는 각각 1.4억 개의 뉴런과 그 뉴런들이 수백억 개나 연결된 V1이라고 불리는 ‘1차시각피질(primary visual cortex)’이 자리하고 있다. 점진적으로 고도의 영상 처리를 가능하게 하려면 V2, V3, V4 그리고 V5 같은 시각피질들도 시각시스템에 모두 연루되어야만 한다. 인간의 머릿속에는 수 억년 동안 진화하며 시각 세계를 이해하기 위해 훌륭하게 적응된 슈퍼컴퓨터가 있다. 필기체 숫자를 인식하기는 쉽지 않은데도 우리 인간은 경탄스럽게도 눈이 보는 것을 너무 잘 이해하는 것이다. 이런 과정은 무의식적으로 일어나기 때문에 시각시스템이 얼마나 힘든 문제를 해결하는지 그  진가를 알지 못한다.

위와 같은 숫자를 컴퓨터 프로그램을 작성해서 인식하도록 하면 시각패턴인식이 얼마나 어려운지 명백해진다. 아까는 쉬웠던 일이 갑자기 극도로 어려워지는 것이다. 우리가 어떻게 모양을 인지하는지 간단한 예를 들어보자. “9는 윗쪽에 고리 모양이 있고 오른쪽 아래에는 수직의 획이 있다.” 이것을 알고리즘으로 나타내는 것은 간단하지 않다. 이런 규칙들을 정밀하게 만들려고 시도하면 할수록 예외, 경고, 특별한 사례들로 수렁에 빠질 것이다. 절망적인 순간이다.

뉴럴네트워크는 이 문제를 다른 방식으로 접근한다. 발상은 다음과 같이

![mnist_100_digits](/blog/assets/images/mnist_100_digits.png){: .center-image}

 훈련 사례(training example)라고 부를 수 있는 많은 손글씨 숫자들을 취합하는 것이다. 그다음에 훈련사례들을 가지고 학습할 수 있는 시스템을 만든다. 다시 말하자면, 뉴럴네트워크가 훈련사례들을 이용해서 손글씨 숫자를 인식하는 규칙들을 자동으로 추론하는 것이다. 훈련사례의 개수를 늘리면 뉴럴네트워크는 좀 더 많은 손글씨를 학습할 수 있으므로 결국 추론의 정확도는 올라간다. 여기에선 단지 100개의 훈련사례만 사용했지만 수천, 수백만 개 더 나아가 수십억 개의 훈련사례를 사용하면 손글씨 인식기의 성능을 훨씬 높일 수 있을 것이다.   

이번 장에서 우리는 뉴럴네트워크가 손글씨 숫자를 인식하도록 학습하는 컴퓨터 프로그램을 짤 것이다. 이건 코드가 74줄밖에 안 되고 특별한 뉴럴네트워크 라이브러리도 필요없다. 하지만 이 짧은 프로그램은 사람의 개입 없이 96%의 정확도로 숫자들을 인식할 수 있다. 다음 장들에서는 이 프로그램의 정확도를 점차 99%까지 향상할 것이다. 현재 가장 좋은 유료의 뉴럴네트워크 프로그램들은 카메라 이미지에서 은행의 수표나 편지의 주소를 인식하는 데 쓰일 정도로 성능이 좋다는 것을 알아두자.

당분간 손글씨 인식에 집중하기로 하자. 이것이 뉴럴네트워크를 공부하기 위한 일반적으로 훌륭한 전형적 문제이기 때문이다. 손글씨 인식은 도전적이지만 또한 균형이 잡힌 사례이다. 손글씨 숫자인식은 간단한 것이 아니지만, 해결책이 극도로 복잡하거나 엄청난 계산량이 필요한 만큼 어렵지는 않다. 게다가 딥러닝 같은 고급 기법을 이해하는데 훌륭한 방법이기도 하다. 따라서 이 책에서 반복적으로 손글씨 인식문제를 상기시킬 것이다. 이어 책 후반부에서는 컴퓨터비젼, 음성인식, 자연어 처리 등의 분야에서 이러한 아이디어가 어떻게 적용되는지 논의할 것이다.

컴퓨터 프로그램이 손글씨 숫자를 인식하게 하는 것만이 목표라면, 이번 장은 당연히 훨씬 짧았을 것이다. 하지만 우리는 뉴럴네트워크의 핵심인 퍼셉트론(perceptron)과 시그모이드 뉴런(sigmoid neuron) 같은 두 가지 인공뉴런과 확률적 경사하강법(stochastic gradient descent) 같은 표준학습 알고리즘을 섭렵해야 한다. 나는 핵심 아이디어들을 왜 그렇게 배치했는지 설명하고 뉴럴네트워크에 대한 독자의 직관을 세우는 데 집중할 것이다. 따라서 뉴럴네트워크에서 어떤 일들이 벌어지는지 기본적인 역학을 설명하는 것보다 더 긴 논의가 필요했다. 더 깊은 이해를 위해서는 이것이 바람직하다. 1장을 다 읽고 우리는 딥러닝이 무엇이며 왜 중요한지 이해하게 될 것이다.



##2. 퍼셉트론##

뉴럴네트워크란 무엇일까? 시작하기에 앞서, 인공뉴런의 하나인 '*퍼셉트론(perceptron)*'에 대해 알아보자. 퍼셉트론은 [워렌 맥쿨로치(Warren McCulloch)](https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch)와 [월터 피츠(Walter Pitss)](https://en.wikipedia.org/wiki/Walter_Pitts)의 연구에서 힌트를 얻어 1950~1960년대에 과학자 [프랭크 로젠블래트(Frank Rosenblatt)](https://en.wikipedia.org/wiki/Frank_Rosenblatt)가 개발했다. 그러나, 현재는 또다른 인공뉴런 모델이 일반적으로 쓰인다. - 이 책에서, 그리고 현대 신경망 연구의 대부분에서 *시그모이드 뉴런* 모델을 쓰고 있다. 시그모이드 뉴런에 대해서 곧 공부할 것이다. 하지만 시그모이드 뉴런을 왜 특정한 방식으로 정의했는지 이해하려면 먼저 퍼셉트론부터 들여다 봐야 한다.


그럼 퍼셉트론은 어떻게 작동하는 걸까? 퍼셉트론은 몇 개의 이진수 입력 $$x_1,x_2,\ldots,$$을 받아들이고 한개의 이진수 출력을 가진다:

![perceptron](/blog/assets/images/tikz0.png){: .center-image}

위의 예는 $$x_1,x_2,x_3$$를 입력으로 받는다. 입력 갯수가 이것보다 적거나 많아도 된다. 로젠블래트는 퍼셉트론의 출력을 계산하는 간단한 방법을 제안했다. 각각의 입력이 출력에 대한 상대적 중요도를 실수형의 가중치인 $$w_1,w_2,\ldots,$$등으로 표현하는 것이다. 뉴런의 출력은 가중합$$\sum_j{w_jx_j}$$이 정해진 [문턱값](http://toparadic.tistory.com/495)보다 작으면 0을, 크면 1로 결정된다. 문턱값은 실수로서 뉴런의 매개변수이다. 이를 좀 더 대수적으로 표현하면:

$$
\begin{equation}
output (출력)=
  \begin{cases}
    0       & \quad \text{if } \sum_j{w_jx_j} \le\text{문턱값}\\
    1  		& \quad \text{if } \sum_j{w_jx_j} > \text{문턱값}\\
  \end{cases}
 \end{equation}
$$

이것이 퍼셉트론이 작동하는 원리의 전부이다.


퍼셉트론은 기본적인 수학모델이다. 퍼셉트론은 여러 입력들을 심사숙고하여 결정하는 장치와 같다고 생각하면 된다. 그렇게 현실적인 예는 아니지만 쉬운 예를 들어보자. 나중에 곧 현실적인 예를 보여줄 것이다. 주말이 다가오고 있고, 여러분이 살고 있는 동네에 치즈 축제가 열린다고 가정 해보자. 여러분은 치즈를 좋아하기 때문에 갈지 말지 고민하기 시작했다. 이 때 여러분은 3가지 요소를 가늠해서 결정을 내릴지 모른다.  

1. 날씨가 화창한가?
2. 여러분의 연인이 같이 가고 싶어하는가?
3. 축제가 대중교통 근처인가? (자가용이 없다고 가정)  

위의 세 가지 요소를 $$x_1,x_2,x_3$$이라고 두자. 예를 들어, 날씨가 화창하면 $$x_1=1$$을, 날씨가 나쁘다면 $$x_1=0$$이 된다. 비슷하게, 연인이 같이 가고 싶어하면 $$x_2=1$$을, 같이 가기 싫어하면 $$x_2=0$$이 된다. 마지막으로, 대중 교통이 근처에 있다면 $$x_3=1$$을, 대중 교통이 근처에 없다면 $$x_3=0$$이 된다.

지금 여러분은 치즈를 너무 너무 좋아해서 심지어 연인이 축제에 관심이 없고 마땅히 이용할 교통수단이 없어도 축제에 가고 싶다고 가정하자. 그렇지만 여러분은 혹시 나쁜 날씨를 정말 싫어하는 성격일지도 모른다. 이런 종류의 의사결정은 퍼셉트론으로 가능하다. 의사결정의 한가지 방법은 날씨 가중치를 $$w_1=6$$로 두고 다른 가중치를 각각 $$w_2=2$$와 $$w_3=2$$로 놓는 것이다. 두 가중치보다 $$w_1$$이 크므로 날씨가 연인이나 대중교통 접근성보다 훨씬 중요하다는 사실을 나타낸다. 마지막으로, 문턱값을 5로 정했다고 가정하자. 이 선택에서 퍼셉트론은 날씨가 화창하면 출력은 언제나 1이 되고, 나쁘면 출력은 0이 된다. 그렇다면, 연인의 선택과 대중교통의 접근성이 결과에 전혀 영향을 미치지 않는 것이다.

가중치와 문턱값을 다양하게 조정해보면 우리는 다른 의사결정 모델을 얻게 된다. 예를 들어, 문턱값을 3으로 골랐다고 가정하자. 그렇다면 퍼셉트론은 우리에게 날씨가 화창할때는 언제든지, 아니면 대중교통 접근성이 좋고 연인이 같이 가길 원할때면 축제에 가라고 말한다. 이건 앞선 것과 또 다른 의사결정 모델이다. 문턱값을 내림으로서 축제에 가고싶은 욕구를 더 나타내는 것이다.  


당연히, 퍼셉트론은 인간의 의사결정에 이써 완전한 모델은 아니다! 하지만 위의 예에서 퍼셉트론이 의사 결정을 내리기 위해 어떻게 다른 종류의 정황을 저울질 하는지 보여주었다. 좀 더 복잡한 퍼셉트론 네트워크를 사용하면 더 예리한 결정도 가능할 듯 하다.

![complex perceptron](/blog/assets/images/tikz1.png){: .center-image}

위의 네트워크에서, 첫번째 열의 퍼셉트론들은 여러 입력들을 저울질하여 3개의 매우 간단한 결정을 내린다. 이 때, 첫번째 열의 각 퍼셉트론을 첫번째 퍼셉트론 층(layer)라고 부를 것이다. 두번째 퍼셉트론 층에서는 무엇을 하는가? 첫번째 층에 있는 결과들을 이 곳의 퍼셉트론들이 다시 저울질하여 의사결정을 만든다. 두번째 층에서는 첫번째 층에서 이루어진 의사결정 레벨보다 한층 복잡하고 추상적인 결정을 내릴 수 있다. 세번째 퍼셉트론 층은 그보다 훨씬 복잡한 의사결정도 가능하다. 이런 식으로, 퍼셉트론의 다층(many-layer) 네트워크는 수준높은 의사결정 문제의 해결도 가능하다.

그런데, 아까 퍼셉트론을 정의할 때 그것이 하나의 출력만을 가진다고 말했다. 위의 네트워크에서 퍼셉트론의 출력은 여러개처럼 보임에도 말이다. 사실, 그것들은 여전히 하나의 출력이다. 각각의 출력 화살표들은 다른 퍼셉트론들의 입력으로 사용되는 것을 가리킬 때만 유용하다. 단일한 출력 선을 그린 후 그것을 다시 여러개로 갈라지게 하는 것보다 덜 복잡하기 때문이다.

이제 간단한 방식으로 퍼셉트론이 무엇인지 서술해보자. $\sum_j{w_jx_j}=문턱값$이라는 조건은 복잡해보이므로 표기법에 두가지 변화를 주었다. 첫째로, $\sum_j{w_jx_j}$를 내적값 $w\cdot x \equiv \sum_j{w_jx_j}$으로 바꾸는 것이다. 여기에서 $w$와 $x$는 각각 가중치와 입력 벡터이다. 둘째로, (1)의 식에 있던 문턱치를 왼쪽편으로 넘겨버리는 것이다. 이 때 퍼셉트론의 편향치(bias) $b$는 음의 문턱값과 동일하다. 문턱값 대신에 편향치를 이용해서 퍼셉트론 규칙을 다시 써보면 다음과 같다.

$$
\begin{eqnarray}
\mbox{출력} = \left\{ \begin{array}{ll} 0 & \mbox{if } w\cdot x + b \leq 0 \\ 1 & \mbox{if } w\cdot x + b > 0 \end{array} \right. \tag{2}\end{eqnarray}
$$

여기에서, 편향치는 퍼셉트론이 1인 출력을 얻는 것이 얼마나 쉬운지 척도를 말해준다. 생물학적인 용어로 말하자면, 편향치는 퍼셉트론이 얼마나 쉽게 *발화* 하는 지의 척도이다. 굉장히 큰 편향치를 가진 퍼셉트론의 경우, 퍼셉트론이 1의 출력을 만드는 것은 너무 쉽다. 하지만 만약 편향치가 너무 큰 음수라면, 출력을 1로 만드는 것은 어렵다. 여기선 비록 편향치가 퍼셉트론을 설명하는 비중이 작지만, 나중에 이것이 수식을 더욱 간단하게 만들어 줄 것이다. 이것 때문에, 책의 나머지 부분에서, 우리는 언제나 편중치를 문턱값 대신 사용할 것이다.


나는 퍼셉트론이 입력들을 가중치에 따라 저울질하여 의사결정하는 방법이라고 소개했다. 우리는 퍼셉트론을 AND, OR, 그리고 NAND같은 기초적인 논리 함수를 계산하는데도 사용할 수 있다. 예를 들어, 아래의 그림처럼 각각의 가중치가 모두 $-2$이고 총 편향치는 $3$인 두개의 입력을 가진 퍼셉트론이 있다고 하자.

![complex perceptron](/blog/assets/images/tikz2.png){: .center-image}

여기서, 입력 $00$을 계산하면 $\(-2\)\times 0 + \(-2\)\times 0 + 3 = 3$이 양수이므로 출력은 1이 된다. 입력 $01$과 $10$도 $1$을 출력한다. 하지만, 입력 $11$을 계산하면 $\(-2\)\times 1 + \(-2\)\times 1 + 3 = -1$ 음수이므로 $0$을 출력한다. 자, 이제 퍼셉트론을 가지고 NAND게이트를 만들어 보자!

NAND만들기 예제는 퍼셉트론을 간단한 논리함수 계산에 이용할 수 있다는 것을 보여준다. 실제로, 퍼셉트론은 어떤 논리함수든 계산할 수 있다. 그 이유는 NAND게이트는 모든 계산에서 보편적 요소이고, 그 말은 NAND를 가지고 어떤 계산식도 세울 수 있다는 이야기다. 예를 들어, NAND게이트로 $x_1$과 $x_2$를 더하는 회로를 만들 수 있다. 이 때, $x_1$과 $x_2$가 모두 $1$이면 윗자리로 1을 넘겨주는 비트올림(carry bit)이 가능해야 하고, 또한 비트단위합 $x_1\oplus x_2$도 필요하다. 비트올림은 그저 비트단위곱인 $x_1x_2$다.

![NAND gate addition](/blog/assets/images/add.png){: .center-image}

두 개의 입력을 가중치를 $-2$ 그리고 총 편향치를 $3$으로 세팅한 퍼셉트론은 위의 연산과 같다. 아래의 네트워크가 결과를 보여준다. 도표를 보면, 오른쪽 아래 NAND 게이트에 해당하는 퍼셉트론을 화살표를 쉽게 그리기 위해 살짝 옮겨 놓았다.

![complex perceptron](/blog/assets/images/tikz4.png){: .center-image}

이 퍼셉트론 네트워크에서 한가지 주목할 만한 점은 가장 왼쪽에 있는 퍼셉트론에서 나온 출력이 가장 아래에 있는 퍼셉트론에 두번이나 입력된다는 것이다.  퍼셉트론 모델을 정의할때, 나는 이런 중복 입력이 가능한지 말하지 않았다. 사실, 이건 별 문제가 아니다. 만약 이런 종류의 입력을 차단하려면, 가중치가 각각 $2$인 두개의 선을 가중치가 $4$인 하나의 선으로 합치면 된다. (잘 이해가 안된다면, 하던 걸 멈추고 이것이 맞는지 스스로 증명해 보라.) 이렇게 바꾸면, 가중치를 표시하지 않은 선의 가중치는 모두 $-2$이고 편향치는 $3$인 네트워크와 같음을 볼 수 있다.

![complex perceptron](/blog/assets/images/tikz5.png){: .center-image}

지금까지 우리는 $x_1$이나 $x_2$와 같은 입력을 퍼센트론 네트워크의 왼쪽에서 떠다니는 변수로 그려왔다. 사실, 입력은 '입력 층'이라고 부르는 별도의 퍼셉트론 층으로 표현하는 방법이 좀 더 일반적이다.

![complex perceptron](/blog/assets/images/tikz6.png){: .center-image}

따라서, 출력은 있지만 입력은 없는 입력 퍼셉트론은  다음과 같이 약식으로 표기할 수 있다.

![complex perceptron](/blog/assets/images/tikz7.png){: .center-image}

이건 입력이 없는 퍼셉트론을 뜻하지는 않는다. 이걸 이해하려면 입력이 없는 퍼셉트론을 가정해보아야 한다. 그렇다면 가중합 $\sum_j w_j x_j$은 언제나 0이 되고, $b > 0$라면 퍼셉트론은 출력을 $1$로, $b \leq 0$면 출력을 $0$으로 가질 것이기 때문이다. 즉, 퍼셉트론은 당연히 나오는 출력값(여기서는 $x_1$)이 아니라 항상 고정된 값만 출력할 것이다. 따라서, 입력 퍼셉트론은 진짜 퍼셉트론이 아니라 $x1$, $x2$등과 같은 입력을 단순히 출력해주는 특별한 유닛(unit)으로 간주해야 한다.

위의 덧셈 예제에서는 퍼셉트론 네트워크를 가지고 여러개의 NAND 게이트를 가진 회로를 시뮬레이션 하는 방법을 보여주었다. NAND 게이트가 계산보편성이 있으므로, 당연히 퍼셉트론도 그렇다.

퍼셉트론의 계산보편성은 장점과 단점을 동시에 보여준다. 퍼셉트론 네트워크가 다른 계산장치들 만큼이나 강력하다는 것은 좋은 소식이다. 하지만, 한편으로는 이것이 새로운 NAND 게이트의 종류에 불과해보여 실망스러울 지 모른다. 그건 결코 좋은 소식이 아니다!

그러나 실제로는 위의 관점보다 낙관적이다. *학습 알고리즘*을 고안해서 인공뉴런 네트워크의 가중치와 편향치를 자동으로 조정(tuning) 줄 수 있기 때문이다. 이 조정은 프로그래머의 직접적인 개입없이 외부의 자극에 반응하여 일어난다. 이런 학습 알고리즘은 전통적인 논리게이트와 근본적으로 다른 방식으로 인공뉴런의 사용을 가능케 해준다. 결론적으로, 전통적 회로설계법을 이용하여 NAND및 여타 게이트들을 회로위에 직접 깔아두고 문제를 푸는 대신에, 뉴럴네트워크는 극도로 어려운 문제도 쉽게 풀 수 있다.



## 3. 시그모이드 뉴런###

학습 알고리즘이라고 하니 좀 대단해 보인다. 하지만, 어떻게 뉴럴네트워크에다 이런 알고리즘을 적용할 수 있을까? 우리가 퍼셉트론을 어떤 문제를 해결하기 위해 사용한다고 가정해 보자. 예를들어, 손글씨 숫자를 스캔하여 픽셀 데이터를 얻고 그것을 네트워크의 입력으로 넣었다고 말이다. 그렇다면, 네트워크가 가중치와 편향치를 학습하여 스캔된 숫자를 정확히 분류하길 원할 것이다. 학습이 어떻게 작동하는지 보기 위해, 네트워크에서 가중치나 편향치를 살짝 변화시켜 보자. 가중치를 살짝 변경해서 네트워크에서 출력도 그에 맞게 적당히 변화하길 바라는 것이다. 금방 알게 되겠지만, 이런 속성이 학습을 가능하게 한다. 이것이 개략적으로 우리가 원하는 것이다 (당연히, 이 네트워크를 손글씨 인식에 쓰기엔 너무 단순하다).

 ![handwritten digits](/blog/assets/images/tikz8.png){: .center-image}

만약 가중치나 편향치를 살짝 변화시킬때 출력도 조금만 변화될 뿐이라면, 이 사실로 볼때 좀 더 우리가 원하는 방향으로 네트워크가 작동하도록 가중치나 편향치를 변경시킬 수도 있을 이다. 예를 들어, 네트워크가 이미지를 숫자 "$9$"를 "$8$"이라고 잘못 분류했다고 가정하자. 이 때 이미지를 분류하여 숫자 "$9$"에 조금씩 가까워지도록 하기 위해  어떻게 가중치와 편향치를 변경해야 하는지가 문제이다. 조금씩 가중치와 편향치를 변화하여 반복하게 되면 점점 더 좋은 결과를 얻게된다. 이것이 네트워크가 학습하는 원리이다.


그런데 네트워크가 퍼셉트론으로 이루어져 있다면 위의 학습이 불가능하다. 당연하게도, 가중치와 편향치를 조금 건드리면 네트워크의 결과가 급격히 변동한다. 사실, $0$에서 $1$처럼 급격하게 바뀐다. 이러한 급격한 변동은 네트워크의 나머지 부분도 완전히 이해하기 어렵고 복잡하게 변하게 한다. 그래서 이미지가 숫자 "9"로 올바로 분류가 되는 한편, 다른 숫자 이미지를 인지하는 네트워크의 작용은 통제하기 어렵게 변할지도 모른다. 이 때문에,  네트워크가 가중치와 편향치를 점진적으로 변화시키는 방법이 좋은 결과로 수렴하기 어렵게 한다. 다행이 이러한 문제를 성공적으로 해결할 방법이 있긴 하다. 하지만, 퍼셉트론으로 이것을 어떻게 해결할 지 지금 당장은 불명확해 보인다.


이 문제는 새로운 인공뉴런의 종류인 시그모이드 뉴런(sigmoid neuron)으로 해결할 수 있다. 시그모이드 뉴런은 퍼셉트론과 비슷하지만, 가중치와 편향치를 약간 변동시키면 출력에도 약간의 변화만을 일으키도록 개선되었다. 이 사실이 퍼셉트론에 비해 시그모이드 뉴런의 네트워크가 더 잘 학습하는 중대한 요인이다.


좋다, 그럼 시그모이드 뉴런을 묘사해보자. 시그모이드 뉴런도 퍼셉트론을 그렸던 방식처럼 그려보자.

 ![handwritten digits](/blog/assets/images/tikz9.png){: .center-image}

시그모이드 뉴런도 퍼셉트론처럼 $x_1$, $x_2,\ldots$같은 입력을 받는다. 그렇지만 $0$과 $1$ 대신에 그 사이값의 입력도 가능하다. 그래서 $0.638\ldots$ 같은 값도 시그모이드 뉴런에서는 유효한 입력이다. 또한, 시그모이드 뉴런에는 $w_1, w_2, \ldots$와 $b$ 같은 가중치와 총 편향치가 존재한다. 하지만 출력은 $0$이나 $1$ 아니다. 츨력은 실수로서 $\sigma(w \cdot x+b)$의 값을 가지는데, 여기서 $\sigma$는 *시그모이드 함수*이다. 시그모이드 함수의 정의는 다음과 같다:

\begin{eqnarray}
\label{eq:sig_func}
 \sigma(z) \equiv \frac{1}{1+e^{-z}}. \tag{3}\end{eqnarray}

위 수식을 명시적으로 보이자면, 입력 $x_1, x_2,\ldots$, 가중치 $w_1, w_2,\ldots$ 그리고 총 편향치 $b$를 가진 시그모이드 뉴런은 아래와 같다.

\begin{eqnarray}
\label{eq:sig_func_2}
 \frac{1}{1+\exp(-\sum_j w_j x_j-b)}. \tag{4}\end{eqnarray}

처음 시그모이드 뉴런의 수식을 보면 퍼셉트론과는 매우 달라보인다. 시그모이드 함수의 대수적(algebraic) 형태는 이것에 익숙한 사람이 아니라면 뭔가 불투명하고 어지럽게 보일지 모른다. 당신이 이미 친숙한 경우가 아니라면 접근하기 어려워 보일지도 모른다. 복잡한 수식이 이해를 막는 장벽이 될 수는 없다. 왜냐하면 두 자기 형태의 뉴런 사이에는 많은 공통점이 있고, 시그모이드 함수의 대수적 형태가 퍼셉트론보다 좀 더 많은 기술적인 세부사항이 있는 것 뿐이다.

퍼셉트론과의 유사성을 이해하기 위해, $z \equiv w \cdot x + b$으로 표시되는 $z$가 큰 양수라고 가정해보자. 그렇다면 $e^{-z}\approx 0$가 되며 $\sigma(z)\approx 1$이 된다. 다시 말해, $z$가 큰 양수라면, 시그모이드 뉴런의 출력은 대략 $1$이므로 퍼셉트론의 출력인 $1$과 매우 비슷해진다. 반대로, $z$값이 큰 음수라면 어떨까. 그렇다면 $e^{-z}\approx \infty$가 되며 $\sigma(z)\approx 0$이 된다. 그래서 만약 $z$가 큰 음수라면, 시그모이드 뉴런의 출력은 대략 $0$이므로 퍼셉트론의 출력인 $0$과 매우 비슷해진다. 퍼셉트론 모델과 큰 편차가 생기는 지점은 오로지 $w \cdot x+b$가 적당한 값을 가지는 구간 뿐이다.


그렇다면 $\sigma$의 대수적 형태는 어떠한가? 그것은 어떻게 이해할 수 있을까? 사실,  $\sigma$의 정확한 형태보다 정말 중요한 것은 함수의 플롯(plot) 형태이다. 함수의 플롯은 함수의 좌표를 찍었을 때 그려지는 모양이다. 여기 아래 그림은 함수를 플롯한 것이다:


<div id="sigmoid_graph"><a name="sigmoid_graph"></a></div>
<script src="http://d3js.org/d3.v3.min.js"></script>
<script>
function s(x) {return 1/(1+Math.exp(-x));}
var m = [40, 120, 50, 120];
var height = 290 - m[0] - m[2];
var width = 600 - m[1] - m[3];
var xmin = -5;
var xmax = 5;
var sample = 400;
var x1 = d3.scale.linear().domain([0, sample]).range([xmin, xmax]);
var data = d3.range(sample).map(function(d){ return {
        x: x1(d),
        y: s(x1(d))};
    });
var x = d3.scale.linear().domain([xmin, xmax]).range([0, width]);
var y = d3.scale.linear()
                .domain([0, 1])
                .range([height, 0]);
var line = d3.svg.line()
    .x(function(d) { return x(d.x); })
    .y(function(d) { return y(d.y); })
var graph = d3.select("#sigmoid_graph")
    .append("svg")
    .attr("width", width + m[1] + m[3])
    .attr("height", height + m[0] + m[2])
    .append("g")
    .attr("transform", "translate(" + m[3] + "," + m[0] + ")");
var xAxis = d3.svg.axis()
                  .scale(x)
                  .tickValues(d3.range(-4, 5, 1))
                  .orient("bottom")
graph.append("g")
    .attr("class", "x axis")
    .attr("transform", "translate(0, " + height + ")")
    .call(xAxis);
var yAxis = d3.svg.axis()
                  .scale(y)
                  .tickValues(d3.range(0, 1.01, 0.2))
                  .orient("left")
                  .ticks(5)
graph.append("g")
    .attr("class", "y axis")
    .call(yAxis);
graph.append("path").attr("d", line(data));
graph.append("text")
     .attr("class", "x label")
     .attr("text-anchor", "end")
     .attr("x", width/2)
     .attr("y", height+35)
     .text("z");
graph.append("text")
        .attr("x", (width / 2))             
        .attr("y", -10)
        .attr("text-anchor", "middle")  
        .style("font-size", "16px")
        .text("시그모이드 함수");
</script>

<p>이 형태는 스텝함수(step function)처럼 원본 함수를 평평하게 만든 것이다:
</p>
<div id="step_graph"></div>
<script>
function s(x) {return x < 0 ? 0 : 1;}
var m = [40, 120, 50, 120];
var height = 290 - m[0] - m[2];
var width = 600 - m[1] - m[3];
var xmin = -5;
var xmax = 5;
var sample = 400;
var x1 = d3.scale.linear().domain([0, sample]).range([xmin, xmax]);
var data = d3.range(sample).map(function(d){ return {
        x: x1(d),
        y: s(x1(d))};
    });
var x = d3.scale.linear().domain([xmin, xmax]).range([0, width]);
var y = d3.scale.linear()
                .domain([0,1])
                .range([height, 0]);
var line = d3.svg.line()
    .x(function(d) { return x(d.x); })
    .y(function(d) { return y(d.y); })
var graph = d3.select("#step_graph")
    .append("svg")
    .attr("width", width + m[1] + m[3])
    .attr("height", height + m[0] + m[2])
    .append("g")
    .attr("transform", "translate(" + m[3] + "," + m[0] + ")");
var xAxis = d3.svg.axis()
                  .scale(x)
                  .tickValues(d3.range(-4, 5, 1))
                  .orient("bottom")
graph.append("g")
    .attr("class", "x axis")
    .attr("transform", "translate(0, " + height + ")")
    .call(xAxis);
var yAxis = d3.svg.axis()
                  .scale(y)
                  .tickValues(d3.range(0, 1.01, 0.2))
                  .orient("left")
                  .ticks(5)
graph.append("g")
    .attr("class", "y axis")
    .call(yAxis);
graph.append("path").attr("d", line(data));
graph.append("text")
     .attr("class", "x label")
     .attr("text-anchor", "end")
     .attr("x", width/2)
     .attr("y", height+35)
     .text("z");
graph.append("text")
        .attr("x", (width / 2))             
        .attr("y", -10)
        .attr("text-anchor", "middle")  
        .style("font-size", "16px")
        .text("스텝함수");
</script>


만약 $\sigma$가 스텝함수였다면, 시그모이드 뉴런은 퍼셉트론과 같다. 왜냐하면, 시그모이드 뉴런의 출력을 1이나 0으로 결정하는 요인은 $w \cdot x + b$값이 양수인지 음수인지에만 관련있기 때문이다. 하지만 원본 $\sigma$ 함수를 사용하면서 퍼셉트론을 매끈한 모양으로 만들 수 있다. 따라서 세부적인 형태보다는 이러한 매끈함이 $\sigma$ 함수의 핵심이다. $\sigma$의 매끈함은 작은 가중치 변화량 $\Delta w_j$와 작은 편향치 변화량 $\Delta b$이 뉴런에서 작은 출력 변화 $\Delta \mbox{output}$를 줄 수 있다는 것을 의미한다. 미분을 이용하면 $\Delta \mbox{output}$이 대략 아래와 같이 근사된다.

\begin{eqnarray}
\label{eq:delta_output}
  \Delta \mbox{output} \approx \sum_j \left(\frac{\partial \, \mbox{output}}{\partial w_j}
  \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b\right).
\tag{5}\end{eqnarray}



 여러분이 편도함수(partial derivatives)에 익숙하지 않다고 해서 두려워 하지 마시라!  위 수식은 편미분이 등장해 복잡해보이지만, 사실 매우 간단한 걸 이야기해 준다 (좋은 소식이다): $\Delta \mbox{output}$는 $\Delta w_j$와 $\Delta b$의 선형 함수ㅔ 불과하다. 이러한 선형성은 가중치와 편향치를 살짝 변화시켜서 우리가 희망하는 어떤 작은 변화를 출력에서 얻기 쉽다는 것을 의미한다. 따라서, 시그모이드 뉴런이 퍼셉트론의 형태와 질적으로 많이 비슷해보일지라도, 원하는 출력을 얻기 위해 가중치와 편향치를 바꾸는 방법을 알아내기 훨씬 쉽다.

$\sigma$ 함수 본연의 형식이 아니라 플롯 모양이 훨씬 중요하다면, 왜 우리는 수식 \eqref{eq:sig_func}은 어떻게 나오게 된걸까? 실제로, 우리는 이따금씩 몇몇 다른 활성화 함수(activation function)들인 $f(\cdot)$에 대하여 출력이 $f(w \cdot x + b)$인 뉴런을 다룰 것이다다. 다른 활성화 함수를 사용할때 가장 큰 변화는 수식 \eqref{eq:delta_output}에 있는 특정한 변수에 대한 편미분값의 변화이다. 나중에 알게되겠지만, $\sigma$함수를 이용해서 이러한 편도함수를 계산하는 것은 지수함수의 미분에서 특별한 성질을 이용하면 간단하다. 어쨌든, $\sigma$함수는 뉴럴네트워크에서 보편적으로 쓰이며, 이 책에서 가장 많이 등장하는 활성화 함수다.

시그모이드 뉴런의 출력을 어떻게 해석해야 할까? 퍼셉트론과 시그모이드 뉴런의 가장 큰 차이는 시그모이드 뉴런의 출력이 $0$과 $1$만이 아니라는 것이다. 출력값이 $0$과 $1$ 사이의 실수이므로 $0.173$이나 $0.689$같은 값들도 타당한 출력이다. 이러한 속성은 매우 유용하다. 예를 들어, 한 이미지에 있는 픽셀값들의 평균 밝기(intensity)를 뉴럴네트워크의 출력으로 나타낼 수 있게 된다. 하지만 이러한 속성도 가끔은 골칫거리가 될 수 있다. 만약 "입력 이미지가 $9$다"나 "입력 이미지가 $9$가 아니다"처럼 두 가지 상태만을 출력으로 하는 네트워크가 있다고 생각해 보자. 당연히, 퍼셉트론처럼 0과 1읠 출력을 준다면 문제는 쉽다. 하지만, 실제로 중간에 있는 출력에 대해 분류할 수 있도록 하나의 규칙처럼 쓰이는 관례(convention)을 정할 수 있다. 예를 들어, $0.5$와 같거나 더 큰 출력이라면 "$9$"로 분류 하고, 작다면 "$9$"가 아닌 것으로 분류하는 것이다. 혼란을 피하기 위해서, 매번 그것이 관례임을 분명하게 선언하고 사용할 것임을 알려드린다.


###연습문제###

- <p style="font-weight: bold"> 퍼셉트론을 모방하는 시그모이드 뉴런, 파트 1</p>
 -- 퍼셉트론 네트워크의 모든 가중치와 편향치에다 양의 상수 $c$를 곱하더라도 네트워크의 거동이 변하지 않음을 보여라.
<br>
- <p style="font-weight: bold"> 퍼셉트론을 모방하는 시그모이드 뉴런, 파트 2</p>
 -- 위의 문제와 설정이 같다고 전제한다.  퍼셉트론 네트워크에 모든 입력이 정해졌다고 가정하자. 실제 입력값은 필요가 없고 그저 고정된 입력이 필요할 뿐이다. 또한, 네트워크에서 어떤 특정한 퍼셉트론으로의 입력 $x$가 $w\cdot x + b \neq 0$을 만족하는 가중치와 편향치들이 있다고 가정하자. 이제 네트워크에서 모든 퍼셉트론 시그모이드 뉴런으로 교체하자. 그리고 모든 가중치와 편향치에다 양의 상수 $c$를를 곱하자. $c\to\infty$라면, 이러한 시그모이드 뉴런 네트워크의 거동은 퍼셉트론 네트워크와 정확히 일치함을 보여라. 퍼셉트론 중 하나가 $w\cdot x + b = 0$인 경우 어떻게 이것이 실패하는지 보여라.
